{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use\n",
    "The datasets library allows you to load and pre-process your dataset in pure Python, at scale. The dataset can be downloaded and prepared in one call to your local drive by using the load_dataset function.\n",
    "\n",
    "For example, to download the persian config, simply specify the corresponding language config name (i.e., \"fa_ir\" for persian):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "fleurs = load_dataset(\"google/fleurs\", \"fa_ir\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the datasets library, you can also stream the dataset on-the-fly by adding a streaming=True argument to the load_dataset function call. Loading a dataset in streaming mode loads individual samples of the dataset at a time, rather than downloading the entire dataset to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "fleurs = load_dataset(\"google/fleurs\", \"fa_ir\", split=\"train\", streaming=True)\n",
    "print(next(iter(fleurs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data.sampler import BatchSampler, RandomSampler\n",
    "fleurs = load_dataset(\"google/fleurs\", \"fa_ir\", split=\"train\")\n",
    "batch_sampler = BatchSampler(RandomSampler(fleurs), batch_size=32, drop_last=False)\n",
    "dataloader = DataLoader(fleurs, batch_sampler=batch_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "fleurs = load_dataset(\"google/fleurs\", \"fa_ir\", split=\"train\")\n",
    "dataloader = DataLoader(fleurs, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Speech Recognition (ASR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "fleurs_asr = load_dataset(\"google/fleurs\", \"fa_ir\")  # for Afrikaans\n",
    "# to download all data for multi-lingual fine-tuning uncomment following line\n",
    "# fleurs_asr = load_dataset(\"google/fleurs\", \"all\")\n",
    "\n",
    "# see structure\n",
    "print(fleurs_asr)\n",
    "\n",
    "# load audio sample on the fly\n",
    "audio_input = fleurs_asr[\"train\"][0][\"audio\"]  # first decoded audio sample\n",
    "transcription = fleurs_asr[\"train\"][0][\"transcription\"]  # first transcription\n",
    "# use `audio_input` and `transcription` to fine-tune your model for ASR\n",
    "\n",
    "# for analyses see language groups\n",
    "all_language_groups = fleurs_asr[\"train\"].features[\"lang_group_id\"].names\n",
    "lang_group_id = fleurs_asr[\"train\"][0][\"lang_group_id\"]\n",
    "\n",
    "all_language_groups[lang_group_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Language Identification\n",
    "LangID can often be a domain classification, but in the case of FLEURS-LangID, recordings are done in a similar setting across languages and the utterances correspond to n-way parallel sentences, in the exact same domain, making this task particularly relevant for evaluating LangID. The setting is simple, FLEURS-LangID is splitted in train/valid/test for each language. We simply create a single train/valid/test for LangID by merging all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "fleurs_langID = load_dataset(\"google/fleurs\", \"all\") # to download all data\n",
    "\n",
    "# see structure\n",
    "print(fleurs_langID)\n",
    "\n",
    "# load audio sample on the fly\n",
    "audio_input = fleurs_langID[\"train\"][0][\"audio\"]  # first decoded audio sample\n",
    "language_class = fleurs_langID[\"train\"][0][\"lang_id\"]  # first id class\n",
    "language = fleurs_langID[\"train\"].features[\"lang_id\"].names[language_class]\n",
    "\n",
    "# use audio_input and language_class to fine-tune your model for audio classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Retrieval\n",
    "Retrieval provides n-way parallel speech and text data. Similar to how XTREME for text leverages Tatoeba to evaluate bitext mining a.k.a sentence translation retrieval, we use Retrieval to evaluate the quality of fixed-size representations of speech utterances. Our goal is to incentivize the creation of fixed-size speech encoder for speech retrieval. The system has to retrieve the English \"key\" utterance corresponding to the speech translation of \"queries\" in 15 languages. Results have to be reported on the test sets of Retrieval whose utterances are used as queries (and keys for English). We augment the English keys with a large number of utterances to make the task more difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "fleurs_retrieval = load_dataset(\"google/fleurs\", \"fa_ir\")  # for Afrikaans\n",
    "# to download all data for multi-lingual fine-tuning uncomment following line\n",
    "# fleurs_retrieval = load_dataset(\"google/fleurs\", \"all\")\n",
    "\n",
    "# see structure\n",
    "print(fleurs_retrieval)\n",
    "\n",
    "# load audio sample on the fly\n",
    "audio_input = fleurs_retrieval[\"train\"][0][\"audio\"]  # decoded audio sample\n",
    "text_sample_pos = fleurs_retrieval[\"train\"][0][\"transcription\"]  # positive text sample\n",
    "text_sample_neg = fleurs_retrieval[\"train\"][1:20][\"transcription\"] # negative text samples\n",
    "\n",
    "# use `audio_input`, `text_sample_pos`, and `text_sample_neg` to fine-tune your model for retrieval"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
